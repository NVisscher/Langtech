\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Language Technology Practical}
\author{Thijs Eker, Niels Visscher, Kenneth Muller, Maaike Los }
\date{June 2017}

\begin{document}

\maketitle

\section{Introduction}



\section{How the system works}
Instead of creating one system that can deal with all questions, we decided to work out three different subsystems that all try to answer the questions, and combine their answers. Because the three systems are essentially different in the way they retrieve their answers, we hope to have more chance to be able to answer many different types of questions. The three subsystems we developed are as follows:
\subsection*{System 1}

\subsection*{System 2}

At the base of this system is a construction that answers "the X of Y" in the following form: A list of entities and properties is given, in which one element in the list is the property of the next element of the list. For example, "the mother of the queen of England" is parsed and stored as $["mother", "queen", "England"]$. Upon evaluation, the queen of England is looked up, after which the outcome is used to find the mother of the retrieved person. No maximum length of this list structure is defined, and multiple answers may be found, which are all used in the next iteration, until the list is empty.\\

The interpretation of questions is based upon regular expressions in Python, by extracting relevant information in this array format, and evaluating it afterwards. Different regex patterns were defined to be able to handle as many different questions as possible.\\

The following types of questions were supported:\\

\begin{itemize}
	\item Generic `What is' questions.
	\item `Who did' questions, like `Who founded Burger King?'
	\item Imperatives, e.g. `List the ingredients of a pizza'
	\item Counting questions, e.g. `How many households does Emmen have?'
	\item `Named after' questions, like `What person was Earl Grey Tea named after?'
\end{itemize}

NLP processing with Spacy was used in some cases, however the system was not made to depend on it too much, as its output appeared slightly unreliable in test runs. Instead, it was used to find lemma forms of words in the input (mainly to make sure the queries contained singular forms instead of plural forms) and to verify that (in the `Who did' and `Imperaties' cases) a word was verb.\\

Counting was done in three different ways. The requested property could be availeble as a number (employees of McDonald's, for example), in which cases the output could be used directly. In a second case, there might be a property called `Number of X', for example in `Number of households'. In the third case, every `X' was stored independently (like the ingredients in bread), and the program had to count them. All these possibilities were tried, and in case more than one of them yielded an answer (for example, `employees' of `McDonald's' returned an answer, but the number of fields could be counted as well) the highest number was returned as an answer.\\

The `Who did' were given special attention as well; In the example question `Who founded Burger King?', we would like to find the `founder' of `Burger King', so after verifying that the second word was indeed a verb, the system tried to convert this verb into the `agent' carrying out the action, by again making minimal use of Spacy to find the lemma form, and trying to find it in a specified list of known agents. If it could not be found, it would try to form the agent by appending an `r' if the lemma ended with an `e', and `er' otherwise, which appears to work for English quite often.\\

\subsection*{System 3}

This system determines what type of question is being asked based on either NLP dependencies or the first word of the sentence. It is capable of distinguishing between four types of questions.\\
First of all, boolean questions, which are questions where either yes or no is the expected answer. These questions can be identified by having a sentence that starts with one of the following verbs: 'be', 'do', 'can' or 'have' (e.g. Is white the color of milk?). We solve these questions by extracting the two entities from the sentence and putting these into an ask query which has the following form:\\
'ASK{ entity1 ?free entity2}'. This will result in a boolean answer which we convert to yes or no.\\
Secondly, we have answers which ask for a location. Location-based questions are identified when a given sentence start with the word 'where' (e.g. Where are the headquarters of the KFC?). The way this was handled was by taking 'location' as the property and then extracting the entity from the question.\\
Thirdly, there are description questions. Description questions are generally structured like 'Who/what is x?', which requires us to search for the wikidata description of the given x (e.g. What is a tomato?). Similarly to how names instead of Q-values are obtained by asking for a label in SPARQL, we can also obtain the description of a given entity in a SPARQL query by adding ?entityDescription to the SELECT statement.\\
Finally, we have the 'x of y' questions, which are generally sentences which provide us with an entity to search for (y) and the property of this entity (x) (e.g. What is the country of origin of spaghetti?).\\
To find the properties and/or entities, we used the NLP dependencies which were obtained through the use of spaCy. These values are different for each of the question types, and they may differ within these types. The dependencies that we used to obtain our properties and entities are based on the questions which can be found on the nestor page. Furthermore, we needed to be able to properly obtain compounds. To do this, for every entity or property that we found, we looked into the subtree provided by spaCy and checked if any of the words in this subtree had a dependency of 'compound' or 'amod'. If this is the case then this should be added to the given entity or property, otherwise it should not.\\
After we have obtained these entities and properties, we have to obtain proper values to send into our SPARQL queries. Originally we were given the choice between an API or anchortexts, but we decided to combine both methods. We compile a list with the top-5 results, such that we end up with a list of at most 10 results, from both the API and the anchortexts. These results are then one-by-one used for our SPARQL query until we obtain an answer. If we cannot obtain an answer after looping through the entire list, we return an empty list.\\
\\
\\
After having all three systems run over a question, we checked whether or not there were two or more systems that gave the same answer. If so, this became our final answer. Else, we looked at respectively system 1, system 2 and system 3 in this order whether they found and answer and return that answer. We chose this order because in our test session, system 1 had the highest percentage of correct answers and system 3 the lowest.  

\section{Evaluation}
Our system was capable of answering 29 out of the 50 test questions properly. The questions we were able to correctly answer were questions in the original format (What are the ingredients of Coca-Cola?), since this is the type of question that we had to be able to answer for assignment 4, all of our systems are well prepared to answer these questions. Furthermore, our system is capable of answering lots of boolean-typed questions where we have to check whether one entity can be found in another entity (do hamburgers contain meat?). Finally, questions where we had to obtain a description of an entity were also answered (Who is Gordon Ramsay?). Our system specifically performed poorly on one question (What is a vegetarian), where an exception was raised. The exact reasoning behind this exception is unknown, since we are actually capable of answering other questions which require us to obtain a description.
Another type of question which our system had difficulties with are questions where the order of a property and an entity are switched around (What does Heineken produce?), since our systems generally search for a property followed by an entity.
We also had difficulties with returning lists of items which all contain a certain property (Which seafood dishes originate from spain), since none of our systems were equipped to answer these questions.

\section{Accountability}
Since we had multiple systems that we had to work on, we divided these amongst ourselves.
Thijs worked on system 1 while Niels worked on system 2 and Maaike and Kenneth worked on system 3.
Furthermore, Thijs and Niels set up the program which allowed us to combine the answers of all of our systems.
Finally, everyone put effort into the report.

\end{document}
